---
title: "Approach"
date-modified: "`r Sys.Date()`"
number-sections: true
format:
  html:
    code-tools: true
editor: visual
---

## Installing packages

```{r}
pacman::p_load(sf, tmap, tidyverse, sfdep, DT, patchwork, ggplot2)
```

## Data Wrangling

### Geospatial Data

To create map plots, we require shape files of Thailand to create the boundaries of each state for map plotting. We used the dataset from HDX accessible [here](https://data.humdata.org/dataset/cod-ab-tha?). As we wanted to analyse patterns across states, ADM1 is used.

```{r}
TH_ADM1<-st_read(dsn="data/geospatial/shapefiles",
                    layer="tha_admbnda_adm1_rtsd_20220121") |> 
  st_transform(24047) |> #Since the coordinate reference system is not set to the CRS that thailand uses, we use st_transform to change it to Thailans CRS (24047).
  arrange(ADM1_EN) |> 
  select(ADM1_EN, geometry)
```

A check for duplicates is done below:

```{r}
TH_ADM1$ADM1_EN[duplicated(TH_ADM1$ADM1_EN)==TRUE] #check for duplicates
```

Since there are no duplicates, we can continue with our analysis!

### Aspatial Data

The necessary datasets we are interested in are read into R's environment below:

```{r}
consumer_location<-read.csv("data/001_lomo_customers_dataset.csv")
products<-read_csv("data/004_lomo_products_dataset.csv")
products_category<-read_csv("data/005_lomo_product_category_name_translation.csv")
consumer_orders<-read.csv("data/006_lomo_orders_dataset.csv")
order_items<-read_csv("data/007_lomo_order_items_dataset.csv")
payment<-read_csv("data/008_lomo_order_payments_dataset.csv")
reviews<-read_csv("data/009_lomo_order_reviews_dataset.csv")
```

First we want to create a dataframe that aggregates consumer orders and payment amount on a state level

```{r}
consumer_orders_payment<-consumer_orders |> 
  inner_join(payment) #joining order and payment info
```

To make sense of the data on a spatial level, we conduct another join with consumer_location to get the location of each order.

```{r}
consumer_orders_location<-consumer_orders_payment |> 
  inner_join(consumer_location, by="customer_id") |> 
  mutate(customer_state=as.factor(customer_state)) #create factor object for ease of plotting graphs

summary(consumer_orders_location)
```

We are interested to see, at the state level, what are number of orders, average payment and total payment statistics. Using the summarise function, we create a new dataframe for analysis:

```{r}
count_location_state<-consumer_orders_location |> 
  group_by(customer_state) |> 
  summarise(no_orders=n(), ave_payment=mean(payment_value), total_payment=sum(payment_value))  |> 
  rename(ADM1_EN=customer_state)

datatable(count_location_state)
```

Our second aspatial dataframe focuses on product specific data. We first obtain the translated product categories by conducting a join betweem "products" and "product_category" we reassign "products" to this data frame.

```{r}
products<-products |> 
  inner_join(products_category,by=c("product_category_name"="product_category_name_portugese")) |> 
  select(product_id, product_category_name_english) |> 
  mutate(product_category_name_english=as.factor(product_category_name_english))
```

Next, we map review and product information to the order dataset

```{r}
product_order_reviews<-order_items |> 
  left_join(products, by="product_id") |> 
  left_join(reviews, by="order_id")
```

Using summarise, we group the dataset via product category and obtain key statistics from the data (no_orders, ave_review, ave_price, total_price).

```{r}
product_order_reviews_summary<-product_order_reviews |> 
  group_by(product_category_name_english) |> 
  summarise(no_orders=n(), ave_review=mean(review_score), ave_price=mean(price), total_price=sum(price)) |> 
  arrange(desc(total_price))

datatable(product_order_reviews_summary)
```

From the data tables generated, we are able to sort the product categories, and quickly see at a glance, which product cateories had the highest order count, revenue generated or highest reviews.

We can include the location dimension by conducting a join with "consumer_orders_location" previously created

```{r}
product_order_reviews_location<-product_order_reviews |> 
  left_join(consumer_orders_location, by="order_id")
```

## Exploratory Data analysis

### Aggregate Indicators

In this section we aim to differentiate the performance of different states. To answer the question of which states should be pioritised when coming up with a marketing strategy.

Currently, we have 3 indicators: - Total Payment - Average Payment - Number of orders

Lets create 3 chloropleth maps corresponding to the different indicators to investigate the spatial patterns.

```{r}
count_location_state_geometry<-count_location_state |> 
  inner_join(TH_ADM1, by="ADM1_EN") |> 
  st_as_sf()

tmap_mode("plot")
plot_total_payment<-tm_shape(count_location_state_geometry |> 
           select(ADM1_EN, total_payment, geometry))+
  tm_fill("total_payment",
          n=6,
          style="equal",
          palette="Blues")+
  tm_borders(alpha = 0.5) +
  tm_layout(main.title=paste("Distribution of total payment"), 
            main.title.position="center",
            main.title.size = 0.8,
            frame=TRUE)+
  tm_scale_bar()+
  tm_grid(alpha=0.2)

plot_ave_payment<-tm_shape(count_location_state_geometry |> 
           select(ADM1_EN, ave_payment, geometry))+
  tm_fill("ave_payment",
          n=6,
          style="equal",
          palette="Blues")+
  tm_borders(alpha = 0.5) +
  tm_layout(main.title=paste("Distribution of ave payment"), 
            main.title.position="center",
            main.title.size = 0.8,
            frame=TRUE)+
  tm_scale_bar()+
  tm_grid(alpha=0.2)

plot_no_orders<-tm_shape(count_location_state_geometry |> 
           select(ADM1_EN, no_orders, geometry))+
  tm_fill("no_orders",
          n=6,
          style="equal",
          palette="Blues")+
  tm_borders(alpha = 0.5) +
  tm_layout(main.title=paste("Distribution of no payment"), 
            main.title.position="center",
            main.title.size = 0.8,
            frame=TRUE)+
  tm_scale_bar()+
  tm_grid(alpha=0.2)

plot_ave_payment
plot_total_payment
plot_no_orders
```

Given that the aggregated data would have more data points, we can conduct some statistical analysis on the possibility of clustering or dispersion of locations with similar results.

To create a LISA map, we first have to create the HCSA object below. "Phuket" is excluded from our analysis as it does not have neighbours and can cause errors in our execution

```{r}
wm_q<-filter(count_location_state_geometry, ADM1_EN!="Phuket")  |> 
  mutate(nb=st_contiguity(geometry),
         wt=st_inverse_distance(nb,
                                 geometry,
                                 scale=1,
                                 alpha=1)) |> 
  select(ADM1_EN, no_orders, geometry, nb, wt)

sum(is.na(wm_q$no_orders))
wm_q$nb

HCSA<- wm_q |>
  mutate(local_Gi=local_gstar_perm(
    no_orders, nb, wt, nsim=99),
    .before=1)  |> #code chunk to compute Gi star value
  unnest(local_Gi)
```

We can condense the HCSA and wm_q to one function below. We repeat the same for the other 2 variables (total_payment) and (ave_payment)

```{r}
HCSA_no_orders<- filter(count_location_state_geometry, ADM1_EN!="Phuket")  |> 
  mutate(nb=st_contiguity(geometry),
         wt=st_inverse_distance(nb,
                                 geometry,
                                 scale=1,
                                 alpha=1)) |> 
  select(ADM1_EN, no_orders, geometry, nb, wt) |>
  mutate(local_Gi=local_gstar_perm(
    no_orders, nb, wt, nsim=99),
    .before=1)  |> #code chunk to compute Gi star value
  unnest(local_Gi)

HCSA_total_payment<- filter(count_location_state_geometry, ADM1_EN!="Phuket")  |> 
  mutate(nb=st_contiguity(geometry),
         wt=st_inverse_distance(nb,
                                 geometry,
                                 scale=1,
                                 alpha=1)) |> 
  select(ADM1_EN, total_payment, geometry, nb, wt) |>
  mutate(local_Gi=local_gstar_perm(
    total_payment, nb, wt, nsim=99),
    .before=1)  |> #code chunk to compute Gi star value
  unnest(local_Gi)

HCSA_ave_payment<- filter(count_location_state_geometry, ADM1_EN!="Phuket")  |> 
  mutate(nb=st_contiguity(geometry),
         wt=st_inverse_distance(nb,
                                 geometry,
                                 scale=1,
                                 alpha=1)) |> 
  select(ADM1_EN, ave_payment, geometry, nb, wt) |>
  mutate(local_Gi=local_gstar_perm(
    ave_payment, nb, wt, nsim=99),
    .before=1)  |> #code chunk to compute Gi star value
  unnest(local_Gi)
```

We can create the LISA plots below:

```{r}
HCSA_no_orders_plot <- 
    tm_shape(HCSA_no_orders) +
    tm_polygons() +
    tm_shape(HCSA_no_orders %>% filter(p_sim <0.05)) +
    tm_fill("gi_star",
            style="equal",
            n=5) +
    tm_borders(alpha = 0.5) +
    tm_layout(main.title = paste("Significant Local Gi for no_orders"),
              main.title.size = 0.8)

HCSA_total_payment_plot <- 
    tm_shape(HCSA_total_payment) +
    tm_polygons() +
    tm_shape(HCSA_total_payment %>% filter(p_sim <0.05)) +
    tm_fill("gi_star",
            style="equal",
            n=5) +
    tm_borders(alpha = 0.5) +
    tm_layout(main.title = paste("Significant Local Gi for total_payment"),
              main.title.size = 0.8)

HCSA_ave_payment_plot <- 
    tm_shape(HCSA_ave_payment) +
    tm_polygons() +
    tm_shape(HCSA_ave_payment %>% filter(p_sim <0.05)) +
    tm_fill("gi_star",
            style="equal",
            n=5) +
    tm_borders(alpha = 0.5) +
    tm_layout(main.title = paste("Significant Local Gi for ave_payment"),
              main.title.size = 0.8)

HCSA_no_orders_plot
HCSA_total_payment_plot
HCSA_ave_payment_plot
```

#### Observations and conclusions:

### Product specific indicators

We can combine our geospatial and product information to also analyse product specific performance across different states in thailand.

With the dataset in place, we can now create a shiny app to loop through the different products and indicators that the user wants to focus on. We first try to create a prototype with the inputs "cool_stuff" as the product category and "no_orders" as the indicator

```{r}
product_order_reviews_location<-product_order_reviews |> 
  left_join(consumer_orders_location, by="order_id") |>
  group_by(product_category_name_english, customer_state) |> 
  summarise(no_orders=n(), total_price=sum(price), ave_price=mean(price)) |> 
  rename(ADM1_EN=customer_state) |> 
  right_join(TH_ADM1, by="ADM1_EN") |> 
  st_as_sf()

i="cool_stuff"
j="no_orders"

# create tmap plot
tmap_mode("plot")
tm_shape(product_order_reviews_location |> 
           filter(product_category_name_english==i) |> 
           select(ADM1_EN, j, geometry))+
  tm_fill(j,
          n=6,
          style="equal",
          palette="Blues")+
  tm_borders(alpha = 0.5) +
  tm_layout(main.title=paste(j, "for", i), 
            main.title.position="center",
            main.title.size=1.2,
            legend.height=0.45,
            legend.width = 0.35,
            frame=TRUE)+
  tm_scale_bar()+
  tm_grid(alpha=0.2)
```

As we can see in the output, it creates a density map as expected. We can extend our understanding to create a Shiny app so as to loop this analysis across all other products and indicators.

A simple Shiny App can be created:

```{r eval=FALSE}
library(shiny)
product_list<-unique(product_order_reviews_location$product_category_name_english)

# Define the UI
ui <- fluidPage(
  selectInput(
    "indicator",
    label="pick an indicator",
    choices=c("no_orders", "ave_price","total_price"),
    selected="no_orders",
    multiple=FALSE
  ),
  selectInput(
    "product",
    label="pick a product category",
    choices=product_list,
    selected="cool_stuff",
    multiple=FALSE
  ),
  # Create a tmap output element
  tmapOutput("my_map"),
  DT::dataTableOutput(outputId = "my_table")
)

# Define the server
server <- function(input, output) {
  dataset<-reactive({
    product_order_reviews_location |>
      filter(product_category_name_english==input$product) |> 
      select(ADM1_EN, input$indicator, geometry)
  })
  # Render the tmap in the output element
  output$my_map <- renderTmap({
    # Create the tmap
    tm_shape(shp=dataset())+
      tm_fill(input$indicator,
          style="quantile",
          palette="Blues")
  })
  
  output$my_table<-DT::renderDataTable({
    DT::datatable(data=dataset())
  })
}

# Run the app
shinyApp(ui, server)
```

### Time-based analysis of product trends

Something else interesting could be to look into whether there has been decline or increase in product specific indicators over time, on the aggregate level.

Through this analysis, we aim to select the best performing products that are on the rise over the years, and to also sieve out the worst performing products.

Similar to the previous cases, we start with analysing an arbritary test scenario and extend our analysis to other cases/products.

```{r}
i="health_beauty"
j="no_order"

specific_product_time<-product_order_reviews |> 
  mutate(shipping_limit_date=as.Date(shipping_limit_date, format = "%d/%m/%Y")) |> 
  mutate(shipping_month=as.Date(format(shipping_limit_date, "%Y-%m-01"))) |> 
  filter(product_category_name_english==i, 
         shipping_month!="2018-09-01") |> #remove due to incomplete dataset to represent sales of the month
  select(price, product_category_name_english, shipping_month) |> 
  group_by(shipping_month) |> 
  summarise(no_order=n(), total_price=sum(price))
```

With the information in place, we fit a linear model onto the data to see how the indicators (no_order, total_price) varies as time (shipping_month) changes.

We can plot the relationship using ggplot.

```{r}
#fitting linear model and extracting slope of line
coef(lm(no_order~shipping_month, data=specific_product_time))["shipping_month"] |> 
  as.numeric()

ggplot(specific_product_time,aes(x=shipping_month, y=no_order))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)

ggplot(specific_product_time,aes(x=shipping_month, y=total_price))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)
```

To generalise this analysis to all products, we can can create 2 functions to obtain the slopes for no_order and total price below, so as to compare across different products.

```{r}
product_order_reviews
determine_slope_no_order <- function(product) {
  
  specific_product_time<-product_order_reviews |> 
  mutate(shipping_limit_date=as.Date(shipping_limit_date, format = "%d/%m/%Y")) |> 
  mutate(shipping_month=as.Date(format(shipping_limit_date, "%Y-%m-01"))) |> 
  filter(product_category_name_english==product, 
         shipping_month!="2018-09-01") |> #remove due to incomplete dataset to represent sales of the month
  select(price, product_category_name_english, shipping_month) |> 
  group_by(shipping_month) |> 
  summarise(no_order=n(), total_price=sum(price))
  
  slope<-coef(lm(no_order~shipping_month, data=specific_product_time))["shipping_month"] |> 
  as.numeric()
  
  result <- ifelse(is.na(slope), 0, slope)
  
  return(result)
}

determine_slope_total_price <- function(product) {
  
  specific_product_time<-product_order_reviews |> 
  mutate(shipping_limit_date=as.Date(shipping_limit_date, format = "%d/%m/%Y")) |> 
  mutate(shipping_month=as.Date(format(shipping_limit_date, "%Y-%m-01"))) |> 
  filter(product_category_name_english==product, 
         shipping_month!="2018-09-01") |> #remove due to incomplete dataset to represent sales of the month
  select(price, product_category_name_english, shipping_month) |> 
  group_by(shipping_month) |> 
  summarise(no_order=n(), total_price=sum(price))
  
  slope<-coef(lm(total_price~shipping_month, data=specific_product_time))["shipping_month"] |> 
  as.numeric()
  
  result <- ifelse(is.na(slope), 0, slope)
  
  return(result)
}

determine_slope_no_order("fashion_children_clothes")
determine_slope_total_price("fashion_children_clothes")
```

Now, we can initialise an empty dataframe with all of the product names and loop this function through all products.

```{r}
product_list<-unique(product_order_reviews_location$product_category_name_english) |>  #previously created 
  na.omit()

df<-data.frame(product_list = character(0), 
           slope_no_order = character(0),
           slope_total_price = character(0))

for (i in product_list){
  slope_no_order=determine_slope_no_order(i)
  slope_total_price=determine_slope_total_price(i)
  new_row <- data.frame(product_list = i, 
                        slope_no_order = slope_no_order,
                        slope_total_price = slope_total_price)
  df <- rbind(df, new_row)
}

df
```

We can find the top 5 and bottom 5 products based on slope_no_order

```{r}
no_order_bottom_5<-df |> 
  arrange(slope_no_order) |> 
  head(5)

no_order_top_5<-df |> 
  arrange(desc(slope_no_order)) |> 
  head(5)

no_order_bottom_5
no_order_top_5
```

We can repeat the same analysis for slope_total_price

```{r}
total_price_bottom_5<-df |> 
  arrange(slope_no_order) |> 
  head(5)

total_price_top_5<-df |> 
  arrange(desc(slope_no_order)) |> 
  head(5)

total_price_bottom_5
total_price_top_5
```

From our results, we can conclude some products that have grown in sales in the past years, and should be pioritised, as well as others which should be depioritised.
