[
  {
    "objectID": "approach/approach.html",
    "href": "approach/approach.html",
    "title": "Approach",
    "section": "",
    "text": "pacman::p_load(sf, tmap, tidyverse, sfdep, DT, patchwork, ggplot2)"
  },
  {
    "objectID": "approach/approach.html#data-wrangling",
    "href": "approach/approach.html#data-wrangling",
    "title": "Approach",
    "section": "2 Data Wrangling",
    "text": "2 Data Wrangling\n\n2.1 Geospatial Data\nTo create map plots, we require shape files of Thailand to create the boundaries of each state for map plotting. We used the dataset from HDX accessible here. As we wanted to analyse patterns across states, ADM1 is used.\n\nTH_ADM1<-st_read(dsn=\"data/geospatial/shapefiles\",\n                    layer=\"tha_admbnda_adm1_rtsd_20220121\") |> \n  st_transform(24047) |> #Since the coordinate reference system is not set to the CRS that thailand uses, we use st_transform to change it to Thailans CRS (24047).\n  arrange(ADM1_EN) |> \n  select(ADM1_EN, geometry)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `/Users/pengyouyun/youyunpeng/lomocase/approach/data/geospatial/shapefiles' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nA check for duplicates is done below:\n\nTH_ADM1$ADM1_EN[duplicated(TH_ADM1$ADM1_EN)==TRUE] #check for duplicates\n\ncharacter(0)\n\n\nSince there are no duplicates, we can continue with our analysis!\n\n\n2.2 Aspatial Data\nThe necessary datasets we are interested in are read into R’s environment below:\n\nconsumer_location<-read.csv(\"data/001_lomo_customers_dataset.csv\")\nproducts<-read_csv(\"data/004_lomo_products_dataset.csv\")\n\nRows: 32340 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): product_id, product_category_name\ndbl (7): product_name_lenght, product_description_lenght, product_photos_qty...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nproducts_category<-read_csv(\"data/005_lomo_product_category_name_translation.csv\")\n\nRows: 71 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): product_category_name_portugese, product_category_name_english\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nconsumer_orders<-read.csv(\"data/006_lomo_orders_dataset.csv\")\norder_items<-read_csv(\"data/007_lomo_order_items_dataset.csv\")\n\nRows: 112650 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): order_id, product_id, seller_id, shipping_limit_date\ndbl (3): order_item_id, price, freight_value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npayment<-read_csv(\"data/008_lomo_order_payments_dataset.csv\")\n\nRows: 103886 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): order_id, payment_type\ndbl (3): payment_sequential, payment_installments, payment_value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nreviews<-read_csv(\"data/009_lomo_order_reviews_dataset.csv\")\n\nRows: 100000 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): review_id, order_id, review_comment_title, review_comment_message, ...\ndbl (1): review_score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst we want to create a dataframe that aggregates consumer orders and payment amount on a state level\n\nconsumer_orders_payment<-consumer_orders |> \n  inner_join(payment) #joining order and payment info\n\nJoining with `by = join_by(order_id)`\n\n\nWarning in inner_join(consumer_orders, payment): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\n\nTo make sense of the data on a spatial level, we conduct another join with consumer_location to get the location of each order.\n\nconsumer_orders_location<-consumer_orders_payment |> \n  inner_join(consumer_location, by=\"customer_id\") |> \n  mutate(customer_state=as.factor(customer_state)) #create factor object for ease of plotting graphs\n\nsummary(consumer_orders_location)\n\n   order_id         customer_id        order_status      \n Length:103886      Length:103886      Length:103886     \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n order_purchase_timestamp order_approved_at  order_delivered_carrier_date\n Length:103886            Length:103886      Length:103886               \n Class :character         Class :character   Class :character            \n Mode  :character         Mode  :character   Mode  :character            \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n order_delivered_customer_date order_estimated_delivery_date payment_sequential\n Length:103886                 Length:103886                 Min.   : 1.000    \n Class :character              Class :character              1st Qu.: 1.000    \n Mode  :character              Mode  :character              Median : 1.000    \n                                                             Mean   : 1.093    \n                                                             3rd Qu.: 1.000    \n                                                             Max.   :29.000    \n                                                                               \n payment_type       payment_installments payment_value      customer_unique_id\n Length:103886      Min.   : 0.000       Min.   :    0.00   Length:103886     \n Class :character   1st Qu.: 1.000       1st Qu.:   56.79   Class :character  \n Mode  :character   Median : 1.000       Median :  100.00   Mode  :character  \n                    Mean   : 2.853       Mean   :  154.10                     \n                    3rd Qu.: 4.000       3rd Qu.:  171.84                     \n                    Max.   :24.000       Max.   :13664.08                     \n                                                                              \n customer_zip_code_prefix customer_city                customer_state \n Length:103886            Length:103886      Roi Et           : 6474  \n Class :character         Class :character   Nakhon Ratchasima: 5818  \n Mode  :character         Mode  :character   Sakon Nakhon     : 5323  \n                                             Si Sa Ket        : 4997  \n                                             Maha Sarakham    : 4375  \n                                             Ubon Ratchathani : 4223  \n                                             (Other)          :72676  \n\n\nWe are interested to see, at the state level, what are number of orders, average payment and total payment statistics. Using the summarise function, we create a new dataframe for analysis:\n\ncount_location_state<-consumer_orders_location |> \n  group_by(customer_state) |> \n  summarise(no_orders=n(), ave_payment=mean(payment_value), total_payment=sum(payment_value))  |> \n  rename(ADM1_EN=customer_state)\n\ndatatable(count_location_state)\n\n\n\n\n\n\nOur second aspatial dataframe focuses on product specific data. We first obtain the translated product categories by conducting a join betweem “products” and “product_category” we reassign “products” to this data frame.\n\nproducts<-products |> \n  inner_join(products_category,by=c(\"product_category_name\"=\"product_category_name_portugese\")) |> \n  select(product_id, product_category_name_english) |> \n  mutate(product_category_name_english=as.factor(product_category_name_english))\n\nNext, we map review and product information to the order dataset\n\nproduct_order_reviews<-order_items |> \n  left_join(products, by=\"product_id\") |> \n  left_join(reviews, by=\"order_id\")\n\nWarning in left_join(left_join(order_items, products, by = \"product_id\"), : Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 96 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\n\nUsing summarise, we group the dataset via product category and obtain key statistics from the data (no_orders, ave_review, ave_price, total_price).\n\nproduct_order_reviews_summary<-product_order_reviews |> \n  group_by(product_category_name_english) |> \n  summarise(no_orders=n(), ave_review=mean(review_score), ave_price=mean(price), total_price=sum(price)) |> \n  arrange(desc(total_price))\n\ndatatable(product_order_reviews_summary)\n\n\n\n\n\n\nFrom the data tables generated, we are able to sort the product categories, and quickly see at a glance, which product cateories had the highest order count, revenue generated or highest reviews.\nWe can include the location dimension by conducting a join with “consumer_orders_location” previously created\n\nproduct_order_reviews_location<-product_order_reviews |> \n  left_join(consumer_orders_location, by=\"order_id\")\n\nWarning in left_join(product_order_reviews, consumer_orders_location, by = \"order_id\"): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 39 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "problemstatement/problemstatement.html",
    "href": "problemstatement/problemstatement.html",
    "title": "Problem Statement",
    "section": "",
    "text": "LoMo is a department store in Portugal and its online operations focus is on a B2B2C model. Small businesses across the country can leverage the platform to connect with customers and utilise its logistics partners to close the loop. For the context of this exercise, we will focus only on the customer side of the business.\nProcesses that are involved in its operations in fulfilling customer orders are highlighted below\n\nCustomer makes a purchase\nSeller is notified to fulfill order\nCustomer receives goods/estimated delivery date is due, a satisfaction survey is pushed\n\nLoMo has decided to enter Asia Pacific in 2015, selecting Thailand as its first entry point.\nThe problem statements are stated below:\n\nWhich product category has the best performance and which has the highest potential?\nIf you were the Chief Marketing Officer, how would you prioritise your (2C) marketing efforts? Would your strategy differ by region (state)? (Visualising your points on a map would help)"
  },
  {
    "objectID": "problemstatement/problemstatement.html#approach",
    "href": "problemstatement/problemstatement.html#approach",
    "title": "Problem Statement",
    "section": "2 Approach",
    "text": "2 Approach\nIn answering the two questions, we conduct our analysis on both aggregate and state-level.\n\n\n\n\n\n\n\nProblem Statement\nSub Questions\n\n\n\n\nWhich product category has the best performance and which has the highest potential?\nOn an aggregate level:\n\nwhich product categories had the highest order count, revenue generated, revenue/order generated, average reviews\n\nOn the state-level:\n\nSpecific to any selected product, are there specific states that performed better in these statistics\nWhen evaluating the state-level performance, is there spatial clustering in the level of performance\n\n\n\nIf you were the Chief Marketing Officer, how would you prioritise your (2C) marketing efforts? Would your strategy differ by region (state)? (Visualising your points on a map would help)\nOn the state level:\n\nWhich states had the highest order count, revenue generated, revenue/order generated, average reviews\nWhen evaluating the state-level performance, is there spatial clustering in the level of performance\n\n\n\n\nTherefore, we are answering the following questions\n\nWhich states should Lomo focus on (with high value consumers)\nWhich product categories on the aggregate level are the best performing\nIf Lomo is looking at a specific product category, which states should Lomo focus their marketing efforts on"
  },
  {
    "objectID": "approach/approach.html#exploratory-data-analysis",
    "href": "approach/approach.html#exploratory-data-analysis",
    "title": "Approach",
    "section": "3 Exploratory Data analysis",
    "text": "3 Exploratory Data analysis\n\n3.1 Aggregate Indicators\nIn this section we aim to differentiate the performance of different states. To answer the question of which states should be pioritised when coming up with a marketing strategy.\nCurrently, we have 3 indicators: - Total Payment - Average Payment - Number of orders\nLets create 3 chloropleth maps corresponding to the different indicators to investigate the spatial patterns.\n\ncount_location_state_geometry<-count_location_state |> \n  inner_join(TH_ADM1, by=\"ADM1_EN\") |> \n  st_as_sf()\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nplot_total_payment<-tm_shape(count_location_state_geometry |> \n           select(ADM1_EN, total_payment, geometry))+\n  tm_fill(\"total_payment\",\n          n=6,\n          style=\"equal\",\n          palette=\"Blues\")+\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=paste(\"Distribution of total payment\"), \n            main.title.position=\"center\",\n            main.title.size = 0.8,\n            frame=TRUE)+\n  tm_scale_bar()+\n  tm_grid(alpha=0.2)\n\nplot_ave_payment<-tm_shape(count_location_state_geometry |> \n           select(ADM1_EN, ave_payment, geometry))+\n  tm_fill(\"ave_payment\",\n          n=6,\n          style=\"equal\",\n          palette=\"Blues\")+\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=paste(\"Distribution of ave payment\"), \n            main.title.position=\"center\",\n            main.title.size = 0.8,\n            frame=TRUE)+\n  tm_scale_bar()+\n  tm_grid(alpha=0.2)\n\nplot_no_orders<-tm_shape(count_location_state_geometry |> \n           select(ADM1_EN, no_orders, geometry))+\n  tm_fill(\"no_orders\",\n          n=6,\n          style=\"equal\",\n          palette=\"Blues\")+\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=paste(\"Distribution of no payment\"), \n            main.title.position=\"center\",\n            main.title.size = 0.8,\n            frame=TRUE)+\n  tm_scale_bar()+\n  tm_grid(alpha=0.2)\n\nplot_ave_payment\n\nLegend labels were too wide. The labels have been resized to 0.61, 0.61, 0.61, 0.61, 0.61, 0.61. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\nplot_total_payment\n\nLegend labels were too wide. The labels have been resized to 0.48, 0.45, 0.45, 0.45, 0.45, 0.41. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\nplot_no_orders\n\nSome legend labels were too wide. These labels have been resized to 0.61, 0.61, 0.61, 0.61, 0.61. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\n\nGiven that the aggregated data would have more data points, we can conduct some statistical analysis on the possibility of clustering or dispersion of locations with similar results.\nTo create a LISA map, we first have to create the HCSA object below. “Phuket” is excluded from our analysis as it does not have neighbours and can cause errors in our execution\n\nwm_q<-filter(count_location_state_geometry, ADM1_EN!=\"Phuket\")  |> \n  mutate(nb=st_contiguity(geometry),\n         wt=st_inverse_distance(nb,\n                                 geometry,\n                                 scale=1,\n                                 alpha=1)) |> \n  select(ADM1_EN, no_orders, geometry, nb, wt)\n\n! Polygon provided. Using point on surface.\n\nsum(is.na(wm_q$no_orders))\n\n[1] 0\n\nwm_q$nb\n\nNeighbour list object:\nNumber of regions: 68 \nNumber of nonzero links: 298 \nPercentage nonzero weights: 6.444637 \nAverage number of links: 4.382353 \n\nHCSA<- wm_q |>\n  mutate(local_Gi=local_gstar_perm(\n    no_orders, nb, wt, nsim=99),\n    .before=1)  |> #code chunk to compute Gi star value\n  unnest(local_Gi)\n\nWe can condense the HCSA and wm_q to one function below. We repeat the same for the other 2 variables (total_payment) and (ave_payment)\n\nHCSA_no_orders<- filter(count_location_state_geometry, ADM1_EN!=\"Phuket\")  |> \n  mutate(nb=st_contiguity(geometry),\n         wt=st_inverse_distance(nb,\n                                 geometry,\n                                 scale=1,\n                                 alpha=1)) |> \n  select(ADM1_EN, no_orders, geometry, nb, wt) |>\n  mutate(local_Gi=local_gstar_perm(\n    no_orders, nb, wt, nsim=99),\n    .before=1)  |> #code chunk to compute Gi star value\n  unnest(local_Gi)\n\n! Polygon provided. Using point on surface.\n\nHCSA_total_payment<- filter(count_location_state_geometry, ADM1_EN!=\"Phuket\")  |> \n  mutate(nb=st_contiguity(geometry),\n         wt=st_inverse_distance(nb,\n                                 geometry,\n                                 scale=1,\n                                 alpha=1)) |> \n  select(ADM1_EN, total_payment, geometry, nb, wt) |>\n  mutate(local_Gi=local_gstar_perm(\n    total_payment, nb, wt, nsim=99),\n    .before=1)  |> #code chunk to compute Gi star value\n  unnest(local_Gi)\n\n! Polygon provided. Using point on surface.\n\nHCSA_ave_payment<- filter(count_location_state_geometry, ADM1_EN!=\"Phuket\")  |> \n  mutate(nb=st_contiguity(geometry),\n         wt=st_inverse_distance(nb,\n                                 geometry,\n                                 scale=1,\n                                 alpha=1)) |> \n  select(ADM1_EN, ave_payment, geometry, nb, wt) |>\n  mutate(local_Gi=local_gstar_perm(\n    ave_payment, nb, wt, nsim=99),\n    .before=1)  |> #code chunk to compute Gi star value\n  unnest(local_Gi)\n\n! Polygon provided. Using point on surface.\n\n\nWe can create the LISA plots below:\n\nHCSA_no_orders_plot <- \n    tm_shape(HCSA_no_orders) +\n    tm_polygons() +\n    tm_shape(HCSA_no_orders %>% filter(p_sim <0.05)) +\n    tm_fill(\"gi_star\",\n            style=\"equal\",\n            n=5) +\n    tm_borders(alpha = 0.5) +\n    tm_layout(main.title = paste(\"Significant Local Gi for no_orders\"),\n              main.title.size = 0.8)\n\nHCSA_total_payment_plot <- \n    tm_shape(HCSA_total_payment) +\n    tm_polygons() +\n    tm_shape(HCSA_total_payment %>% filter(p_sim <0.05)) +\n    tm_fill(\"gi_star\",\n            style=\"equal\",\n            n=5) +\n    tm_borders(alpha = 0.5) +\n    tm_layout(main.title = paste(\"Significant Local Gi for total_payment\"),\n              main.title.size = 0.8)\n\nHCSA_ave_payment_plot <- \n    tm_shape(HCSA_ave_payment) +\n    tm_polygons() +\n    tm_shape(HCSA_ave_payment %>% filter(p_sim <0.05)) +\n    tm_fill(\"gi_star\",\n            style=\"equal\",\n            n=5) +\n    tm_borders(alpha = 0.5) +\n    tm_layout(main.title = paste(\"Significant Local Gi for ave_payment\"),\n              main.title.size = 0.8)\n\nHCSA_no_orders_plot\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nLegend labels were too wide. The labels have been resized to 0.58, 0.60, 0.63, 0.63, 0.63. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\nHCSA_total_payment_plot\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\nLegend labels were too wide. The labels have been resized to 0.58, 0.60, 0.63, 0.63, 0.63. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\nHCSA_ave_payment_plot\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nLegend labels were too wide. The labels have been resized to 0.58, 0.58, 0.60, 0.63, 0.63. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\n\n\n3.1.1 Observations and conclusions:\n\n\n\n3.2 Product specific indicators\nWe can combine our geospatial and product information to also analyse product specific performance across different states in thailand.\nWith the dataset in place, we can now create a shiny app to loop through the different products and indicators that the user wants to focus on. We first try to create a prototype with the inputs “cool_stuff” as the product category and “no_orders” as the indicator\n\nproduct_order_reviews_location<-product_order_reviews |> \n  left_join(consumer_orders_location, by=\"order_id\") |>\n  group_by(product_category_name_english, customer_state) |> \n  summarise(no_orders=n(), total_price=sum(price), ave_price=mean(price)) |> \n  rename(ADM1_EN=customer_state) |> \n  right_join(TH_ADM1, by=\"ADM1_EN\") |> \n  st_as_sf()\n\nWarning in left_join(product_order_reviews, consumer_orders_location, by = \"order_id\"): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 39 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\n\n`summarise()` has grouped output by 'product_category_name_english'. You can\noverride using the `.groups` argument.\n\ni=\"cool_stuff\"\nj=\"no_orders\"\n\n# create tmap plot\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(product_order_reviews_location |> \n           filter(product_category_name_english==i) |> \n           select(ADM1_EN, j, geometry))+\n  tm_fill(j,\n          n=6,\n          style=\"equal\",\n          palette=\"Blues\")+\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=paste(j, \"for\", i), \n            main.title.position=\"center\",\n            main.title.size=1.2,\n            legend.height=0.45,\n            legend.width = 0.35,\n            frame=TRUE)+\n  tm_scale_bar()+\n  tm_grid(alpha=0.2)\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(j)\n\n  # Now:\n  data %>% select(all_of(j))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\n\n\nLegend labels were too wide. The labels have been resized to 0.64, 0.58, 0.53, 0.48, 0.48, 0.48. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\n\nAs we can see in the output, it creates a density map as expected. We can extend our understanding to create a Shiny app so as to loop this analysis across all other products and indicators.\nA simple Shiny App can be created:\n\nlibrary(shiny)\nproduct_list<-unique(product_order_reviews_location$product_category_name_english)\n\n# Define the UI\nui <- fluidPage(\n  selectInput(\n    \"indicator\",\n    label=\"pick an indicator\",\n    choices=c(\"no_orders\", \"ave_price\",\"total_price\"),\n    selected=\"no_orders\",\n    multiple=FALSE\n  ),\n  selectInput(\n    \"product\",\n    label=\"pick a product category\",\n    choices=product_list,\n    selected=\"cool_stuff\",\n    multiple=FALSE\n  ),\n  # Create a tmap output element\n  tmapOutput(\"my_map\"),\n  DT::dataTableOutput(outputId = \"my_table\")\n)\n\n# Define the server\nserver <- function(input, output) {\n  dataset<-reactive({\n    product_order_reviews_location |>\n      filter(product_category_name_english==input$product) |> \n      select(ADM1_EN, input$indicator, geometry)\n  })\n  # Render the tmap in the output element\n  output$my_map <- renderTmap({\n    # Create the tmap\n    tm_shape(shp=dataset())+\n      tm_fill(input$indicator,\n          style=\"quantile\",\n          palette=\"Blues\")\n  })\n  \n  output$my_table<-DT::renderDataTable({\n    DT::datatable(data=dataset())\n  })\n}\n\n# Run the app\nshinyApp(ui, server)\n\n\n\n3.3 Time-based analysis of product trends\nSomething else interesting could be to look into whether there has been decline or increase in product specific indicators over time, on the aggregate level.\nThrough this analysis, we aim to select the best performing products that are on the rise over the years, and to also sieve out the worst performing products.\nSimilar to the previous cases, we start with analysing an arbritary test scenario and extend our analysis to other cases/products.\n\ni=\"health_beauty\"\nj=\"no_order\"\n\nspecific_product_time<-product_order_reviews |> \n  mutate(shipping_limit_date=as.Date(shipping_limit_date, format = \"%d/%m/%Y\")) |> \n  mutate(shipping_month=as.Date(format(shipping_limit_date, \"%Y-%m-01\"))) |> \n  filter(product_category_name_english==i, \n         shipping_month!=\"2018-09-01\") |> #remove due to incomplete dataset to represent sales of the month\n  select(price, product_category_name_english, shipping_month) |> \n  group_by(shipping_month) |> \n  summarise(no_order=n(), total_price=sum(price))\n\nWith the information in place, we fit a linear model onto the data to see how the indicators (no_order, total_price) varies as time (shipping_month) changes.\nWe can plot the relationship using ggplot.\n\n#fitting linear model and extracting slope of line\ncoef(lm(no_order~shipping_month, data=specific_product_time))[\"shipping_month\"] |> \n  as.numeric()\n\n[1] 1.353922\n\nggplot(specific_product_time,aes(x=shipping_month, y=no_order))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(specific_product_time,aes(x=shipping_month, y=total_price))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nTo generalise this analysis to all products, we can can create 2 functions to obtain the slopes for no_order and total price below, so as to compare across different products.\n\nproduct_order_reviews\n\n# A tibble: 113,322 × 14\n   order…¹ order…² produ…³ selle…⁴ shipp…⁵ price freig…⁶ produ…⁷ revie…⁸ revie…⁹\n   <chr>     <dbl> <chr>   <chr>   <chr>   <dbl>   <dbl> <fct>   <chr>     <dbl>\n 1 000102…       1 424473… 48436d… 19/09/…  58.9    13.3 cool_s… 97ca43…       5\n 2 00018f…       1 e5f2d5… dd7ddc… 03/05/… 240.     19.9 pet_sh… 7b07ba…       4\n 3 000229…       1 c77735… 5b5103… 18/01/… 199      17.9 furnit… 0c5b33…       5\n 4 00024a…       1 7634da… 9d7a1d… 15/08/…  13.0    12.8 perfum… f4028d…       4\n 5 00042b…       1 ac6c36… df5603… 13/02/… 200.     18.1 garden… 940144…       5\n 6 00048c…       1 ef92de… 6426d2… 23/05/…  21.9    12.7 housew… 5e4e50…       4\n 7 00054e…       1 8d4f2b… 7040e8… 14/12/…  19.9    11.8 teleph… 0381de…       4\n 8 000576…       1 557d85… 5996cd… 10/07/… 810      70.8 garden… f0733e…       5\n 9 0005a1…       1 310ae3… a416b6… 26/03/… 146.     11.6 health… 67b1ab…       1\n10 0005f5…       1 4535b0… ba143b… 06/07/…  54.0    11.4 books_… 5c0b7e…       4\n# … with 113,312 more rows, 4 more variables: review_comment_title <chr>,\n#   review_comment_message <chr>, review_creation_date <chr>,\n#   review_answer_timestamp <chr>, and abbreviated variable names ¹​order_id,\n#   ²​order_item_id, ³​product_id, ⁴​seller_id, ⁵​shipping_limit_date,\n#   ⁶​freight_value, ⁷​product_category_name_english, ⁸​review_id, ⁹​review_score\n\ndetermine_slope_no_order <- function(product) {\n  \n  specific_product_time<-product_order_reviews |> \n  mutate(shipping_limit_date=as.Date(shipping_limit_date, format = \"%d/%m/%Y\")) |> \n  mutate(shipping_month=as.Date(format(shipping_limit_date, \"%Y-%m-01\"))) |> \n  filter(product_category_name_english==product, \n         shipping_month!=\"2018-09-01\") |> #remove due to incomplete dataset to represent sales of the month\n  select(price, product_category_name_english, shipping_month) |> \n  group_by(shipping_month) |> \n  summarise(no_order=n(), total_price=sum(price))\n  \n  slope<-coef(lm(no_order~shipping_month, data=specific_product_time))[\"shipping_month\"] |> \n  as.numeric()\n  \n  result <- ifelse(is.na(slope), 0, slope)\n  \n  return(result)\n}\n\ndetermine_slope_total_price <- function(product) {\n  \n  specific_product_time<-product_order_reviews |> \n  mutate(shipping_limit_date=as.Date(shipping_limit_date, format = \"%d/%m/%Y\")) |> \n  mutate(shipping_month=as.Date(format(shipping_limit_date, \"%Y-%m-01\"))) |> \n  filter(product_category_name_english==product, \n         shipping_month!=\"2018-09-01\") |> #remove due to incomplete dataset to represent sales of the month\n  select(price, product_category_name_english, shipping_month) |> \n  group_by(shipping_month) |> \n  summarise(no_order=n(), total_price=sum(price))\n  \n  slope<-coef(lm(total_price~shipping_month, data=specific_product_time))[\"shipping_month\"] |> \n  as.numeric()\n  \n  result <- ifelse(is.na(slope), 0, slope)\n  \n  return(result)\n}\n\ndetermine_slope_no_order(\"fashion_children_clothes\")\n\n[1] 0.001067827\n\ndetermine_slope_total_price(\"fashion_children_clothes\")\n\n[1] -0.03939796\n\n\nNow, we can initialise an empty dataframe with all of the product names and loop this function through all products.\n\nproduct_list<-unique(product_order_reviews_location$product_category_name_english) |>  #previously created \n  na.omit()\n\ndf<-data.frame(product_list = character(0), \n           slope_no_order = character(0),\n           slope_total_price = character(0))\n\nfor (i in product_list){\n  slope_no_order=determine_slope_no_order(i)\n  slope_total_price=determine_slope_total_price(i)\n  new_row <- data.frame(product_list = i, \n                        slope_no_order = slope_no_order,\n                        slope_total_price = slope_total_price)\n  df <- rbind(df, new_row)\n}\n\ndf\n\n                              product_list slope_no_order slope_total_price\n1               agro_industry_and_commerce   3.810210e-02       12.18055138\n2                         air_conditioning   1.968002e-02        2.34530182\n3                                     arts   4.920871e-02        2.01906145\n4                    arts_and_craftmanship   1.627698e-02        1.29199455\n5                                    audio   3.947452e-02        7.02250167\n6                                     auto   6.139787e-01       76.31539728\n7                                     baby   4.016770e-01       59.47765041\n8                           bed_bath_table   1.234247e+00      110.71635691\n9                   books_general_interest   6.564175e-02        5.54040117\n10                          books_imported   1.120078e-02        0.99808960\n11                         books_technical   6.208447e-02        3.39205869\n12                       cds_dvds_musicals  -1.114046e-02       -0.42621238\n13                      christmas_supplies   1.746559e-02        1.05068046\n14                               computers  -2.176399e-02      -41.56012573\n15                   computers_accessories   9.528001e-01       93.73894422\n16                          consoles_games   7.494315e-02        7.12953978\n17         construction_tools_construction   2.924484e-01       45.74871516\n18               construction_tools_garden   3.767080e-02        3.29101331\n19               construction_tools_lights   1.851720e-01       22.05558838\n20               construction_tools_safety   4.813769e-02        8.86010694\n21                construction_tools_tools   2.425829e-02        3.37817876\n22                              cool_stuff   1.741625e-01       29.73272008\n23                                 cuisine   1.958413e-03        0.89656654\n24                     diapers_and_hygiene   5.917308e-03        0.15247126\n25                                  drinks   8.154743e-02        5.09224890\n26                           dvds_blue_ray  -8.840118e-04       -1.33179099\n27                             electronics   4.194712e-01       22.29376112\n28                fashion_bags_accessories   1.641563e-01       14.48077947\n29                fashion_children_clothes   1.067827e-03       -0.03939796\n30                 fashion_female_clothing   1.178389e-03       -0.13467730\n31                   fashion_male_clothing  -1.583813e-03       -0.05544249\n32                           fashion_shoes   1.782081e-04        1.08883711\n33                           fashion_sport  -4.980748e-03       -0.23549370\n34                 fashion_underwear_beach   3.058598e-04        0.01618481\n35                         fixed_telephony  -2.595986e-03        5.89582726\n36                                 flowers  -1.754098e-02       -0.48399445\n37                                    food   1.088698e-01        5.59399274\n38                              food_drink   3.229891e-02        1.93709610\n39                       furniture_bedroom   1.115866e-02        2.16136398\n40                         furniture_decor   6.979400e-01       72.54716415\n41                   furniture_living_room   3.748337e-02        5.05998779\n42       furniture_mattress_and_upholstery   8.577224e-03        0.42462206\n43                            garden_tools   3.485418e-01       32.64500059\n44                           health_beauty   1.353922e+00      177.98073020\n45                         home_appliances   1.402755e-01       15.91066848\n46                       home_appliances_2   3.663464e-02       25.26037819\n47                            home_comfort   1.096850e-03       -0.22554256\n48                          home_comfort_2   2.212958e-03       -0.13948444\n49                       home_construction   1.474481e-01       18.11459566\n50                              housewares   7.600512e-02       11.37482888\n51          industry_commerce_and_business   5.719808e-02        7.91928060\n52 kitchen_dining_laundry_garden_furniture   4.119459e-02        7.22558001\n53                     luggage_accessories   5.149448e-02        5.35595131\n54                            market_place  -6.519224e-03       -1.70966865\n55                                   music   6.079375e-03        1.20322790\n56                     musical_instruments   9.056786e-02       25.42220632\n57                        office_furniture   1.267869e-01       21.37481861\n58                          party_supplies   1.594493e-02        0.81038957\n59                               perfumery   3.147633e-01       27.66949600\n60                                pet_shop   2.698416e-01       30.11734080\n61                   security_and_services   1.292257e-18       -0.34275720\n62                  signaling_and_security   5.201162e-02        6.72543500\n63                        small_appliances   5.609768e-02        7.86960220\n64   small_appliances_home_oven_and_coffee   3.704559e-02       19.25570561\n65                          sports_leisure   8.787427e-01      101.98789730\n66                              stationery   3.574767e-01       30.39205209\n67                  tablets_printing_image  -7.094219e-03       -1.49809993\n68                               telephony   4.478937e-01       43.44669522\n69                           theater_photo   2.257842e-02        2.16242504\n70                                    toys   2.692033e-01       31.22161012\n71                           watches_gifts   9.585161e-01      167.90892362\n\n\nWe can find the top 5 and bottom 5 products based on slope_no_order\n\nno_order_bottom_5<-df |> \n  arrange(slope_no_order) |> \n  head(5)\n\nno_order_top_5<-df |> \n  arrange(desc(slope_no_order)) |> \n  head(5)\n\nno_order_bottom_5\n\n            product_list slope_no_order slope_total_price\n1              computers   -0.021763993       -41.5601257\n2                flowers   -0.017540978        -0.4839945\n3      cds_dvds_musicals   -0.011140456        -0.4262124\n4 tablets_printing_image   -0.007094219        -1.4980999\n5           market_place   -0.006519224        -1.7096687\n\nno_order_top_5\n\n           product_list slope_no_order slope_total_price\n1         health_beauty      1.3539221         177.98073\n2        bed_bath_table      1.2342465         110.71636\n3         watches_gifts      0.9585161         167.90892\n4 computers_accessories      0.9528001          93.73894\n5        sports_leisure      0.8787427         101.98790\n\n\nWe can repeat the same analysis for slope_total_price\n\ntotal_price_bottom_5<-df |> \n  arrange(slope_no_order) |> \n  head(5)\n\ntotal_price_top_5<-df |> \n  arrange(desc(slope_no_order)) |> \n  head(5)\n\ntotal_price_bottom_5\n\n            product_list slope_no_order slope_total_price\n1              computers   -0.021763993       -41.5601257\n2                flowers   -0.017540978        -0.4839945\n3      cds_dvds_musicals   -0.011140456        -0.4262124\n4 tablets_printing_image   -0.007094219        -1.4980999\n5           market_place   -0.006519224        -1.7096687\n\ntotal_price_top_5\n\n           product_list slope_no_order slope_total_price\n1         health_beauty      1.3539221         177.98073\n2        bed_bath_table      1.2342465         110.71636\n3         watches_gifts      0.9585161         167.90892\n4 computers_accessories      0.9528001          93.73894\n5        sports_leisure      0.8787427         101.98790\n\n\nFrom our results, we can conclude some products that have grown in sales in the past years, and should be pioritised, as well as others which should be depioritised."
  },
  {
    "objectID": "approach/approach.html#a-more-generic-analysis-of-products",
    "href": "approach/approach.html#a-more-generic-analysis-of-products",
    "title": "Approach",
    "section": "4 A more generic analysis of products",
    "text": "4 A more generic analysis of products\nTime analysis:\n\ni=\"health_beauty\"\nj=\"no_order\"\n\nspecific_product_time<-product_order_reviews |> \n  mutate(shipping_limit_date=as.Date(shipping_limit_date, format = \"%d/%m/%Y\")) |> \n  mutate(shipping_month=as.Date(format(shipping_limit_date, \"%Y-%m-01\"))) |> \n  filter(product_category_name_english==i, \n         shipping_month!=\"2018-09-01\") |> #remove due to incomplete dataset to represent sales of the month\n  select(price, product_category_name_english, shipping_month) |> \n  group_by(shipping_month) |> \n  summarise(no_order=n(), total_price=sum(price))\n\n#fitting linear model and extracting slope of line\ncoef(lm(no_order~shipping_month, data=specific_product_time))[\"shipping_month\"] |> \n  as.numeric()\n\n[1] 1.353922\n\nggplot(specific_product_time,aes(x=shipping_month, y=no_order))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(specific_product_time,aes(x=shipping_month, y=total_price))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe can create 2 functions to obtain the slopes for no_order and total price below:\n\ndetermine_slope_no_order <- function(product) {\n  \n  specific_product_time<-product_order_reviews |> \n  mutate(shipping_limit_date=as.Date(shipping_limit_date, format = \"%d/%m/%Y\")) |> \n  mutate(shipping_month=as.Date(format(shipping_limit_date, \"%Y-%m-01\"))) |> \n  filter(product_category_name_english==product, \n         shipping_month!=\"2018-09-01\") |> #remove due to incomplete dataset to represent sales of the month\n  select(price, product_category_name_english, shipping_month) |> \n  group_by(shipping_month) |> \n  summarise(no_order=n(), total_price=sum(price))\n  \n  slope<-coef(lm(no_order~shipping_month, data=specific_product_time))[\"shipping_month\"] |> \n  as.numeric()\n  \n  result <- ifelse(is.na(slope), 0, slope)\n  \n  return(result)\n}\n\ndetermine_slope_total_price <- function(product) {\n  \n  specific_product_time<-product_order_reviews |> \n  mutate(shipping_limit_date=as.Date(shipping_limit_date, format = \"%d/%m/%Y\")) |> \n  mutate(shipping_month=as.Date(format(shipping_limit_date, \"%Y-%m-01\"))) |> \n  filter(product_category_name_english==product, \n         shipping_month!=\"2018-09-01\") |> #remove due to incomplete dataset to represent sales of the month\n  select(price, product_category_name_english, shipping_month) |> \n  group_by(shipping_month) |> \n  summarise(no_order=n(), total_price=sum(price))\n  \n  slope<-coef(lm(total_price~shipping_month, data=specific_product_time))[\"shipping_month\"] |> \n  as.numeric()\n  \n  result <- ifelse(is.na(slope), 0, slope)\n  \n  return(result)\n}\n\ndetermine_slope_no_order(\"fashion_children_clothes\")\n\n[1] 0.001067827\n\ndetermine_slope_total_price(\"fashion_children_clothes\")\n\n[1] -0.03939796\n\n\nNow, we can initialise an empty dataframe with all of the product names and loop this function through all products.\n\nproduct_list<-unique(product_order_reviews_location$product_category_name_english) #previously created\n\ndf<-data.frame(product_list, 0, 0) |> \n  rename(slope_no_order=X0, slope_total_price=X0.1)\n\nfor (i in product_list){\n  slope_no_order=determine_slope_no_order(i)\n  slope_total_price=determine_slope_total_price(i)\n  df <- rbind(df, data.frame(product_list = i, slope_no_order = slope_no_order, slope_total_price = slope_total_price))\n}\n\ndf"
  }
]